{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7koNPi78d5tB",
        "outputId": "a11383e8-2a10-4c9e-97b9-553289f73d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.5/214.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -r ./requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFbofSgXea68"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit_chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rqzYY6UeOjr"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import(\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "import streamlit as st\n",
        "from streamlit_chat import message\n",
        "\n",
        "# loading the OpenAI api key from .env (OPENAI_API_KEY=\"sk-********\")\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title='You Custom Assistant',\n",
        "    page_icon='🤖'\n",
        ")\n",
        "st.subheader('Your Custom ChatGPT 🤖')\n",
        "\n",
        "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
        "\n",
        "# creating the messages (chat history) in the Streamlit session state\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# creating the sidebar\n",
        "with st.sidebar:\n",
        "    # streamlit text input widget for the system message (role)\n",
        "    system_message = st.text_input(label='System role')\n",
        "    # streamlit text input widget for the user message\n",
        "    user_prompt = st.text_input(label='Send a message')\n",
        "\n",
        "    if system_message:\n",
        "        if not any(isinstance(x, SystemMessage) for x in st.session_state.messages):\n",
        "            st.session_state.messages.append(\n",
        "                SystemMessage(content=system_message)\n",
        "                )\n",
        "\n",
        "    # st.write(st.session_state.messages)\n",
        "\n",
        "    # if the user entered a question\n",
        "    if user_prompt:\n",
        "        st.session_state.messages.append(\n",
        "            HumanMessage(content=user_prompt)\n",
        "        )\n",
        "\n",
        "        with st.spinner('Working on your request ...'):\n",
        "            # creating the ChatGPT response\n",
        "            response = chat(st.session_state.messages)\n",
        "\n",
        "        # adding the response's content to the session state\n",
        "        st.session_state.messages.append(AIMessage(content=response.content))\n",
        "\n",
        "# st.session_state.messages\n",
        "# message('this is chatgpt', is_user=False)\n",
        "# message('this is the user', is_user=True)\n",
        "\n",
        "# adding a default SystemMessage if the user didn't entered one\n",
        "if len(st.session_state.messages) >= 1:\n",
        "    if not isinstance(st.session_state.messages[0], SystemMessage):\n",
        "        st.session_state.messages.insert(0, SystemMessage(content='You are a helpful assistant.'))\n",
        "\n",
        "# displaying the messages (chat history)\n",
        "for i, msg in enumerate(st.session_state.messages[1:]):\n",
        "    if i % 2 == 0:\n",
        "        message(msg.content, is_user=True, key=f'{i} + 🤓') # user's question\n",
        "    else:\n",
        "        message(msg.content, is_user=False, key=f'{i} +  🤖') # ChatGPT response\n",
        "\n",
        "# run the app:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOpviNUFe0ud"
      },
      "outputs": [],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lIKleMgxeXqp"
      },
      "outputs": [],
      "source": [
        "!streamlit run ./project_streamlit_custom_chatgpt.py &> ./logs.txt & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AeUWFtpfJep"
      },
      "outputs": [],
      "source": [
        "!streamlit run ./project_streamlit_custom_chatgpt.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}