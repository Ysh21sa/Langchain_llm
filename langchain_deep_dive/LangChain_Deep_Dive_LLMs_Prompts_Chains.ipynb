{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e1476ee-4cac-4c75-939e-c6872292a6cc",
      "metadata": {
        "id": "8e1476ee-4cac-4c75-939e-c6872292a6cc"
      },
      "source": [
        "# Deep Dive into LangChain\n",
        "## LLMs, Prompt Templates, Caching, Streaming, Chains\n",
        "\n",
        "This notebook uses the latest versions of the OpenAI and LangChain libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7b118e8-d6e2-4deb-b621-c2f90bc8bd83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7b118e8-d6e2-4deb-b621-c2f90bc8bd83",
        "outputId": "a3eba035-b20e-4fd8-8944-f92016a25793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.9/215.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.0/116.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -r ./requirements.txt -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72695b0b-0069-484d-9edd-9b75faaa8307",
      "metadata": {
        "id": "72695b0b-0069-484d-9edd-9b75faaa8307"
      },
      "source": [
        "Download [requirements.txt](https://drive.google.com/file/d/1UpURYL9kqjXfe9J8o-_Dq5KJTbQpzMef/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57410c7b-edb3-4843-98b5-d71dd2d5bf4e",
      "metadata": {
        "id": "57410c7b-edb3-4843-98b5-d71dd2d5bf4e"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -q langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc5a125-4737-4ab8-ba14-7d4af2685e1b",
      "metadata": {
        "id": "8cc5a125-4737-4ab8-ba14-7d4af2685e1b"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -q openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5444b4-384d-440e-a9a3-2b4c3b5f837c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e5444b4-384d-440e-a9a3-2b4c3b5f837c",
        "outputId": "f404071c-5ee3-4354-a89b-9f9a51742a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: openai\n",
            "Version: 1.23.6\n",
            "Summary: The official Python library for the openai API\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: OpenAI <support@openai.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\n",
            "Required-by: langchain-openai\n"
          ]
        }
      ],
      "source": [
        "!pip show openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06fd9c8e-65ce-4422-b733-ee14876b2614",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06fd9c8e-65ce-4422-b733-ee14876b2614",
        "outputId": "2487eea5-35cf-476b-b075-5a686821cb14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.1.16\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: langchain-experimental\n"
          ]
        }
      ],
      "source": [
        "!pip show langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee9a618-d423-4cce-8e32-ed937698c0d5",
      "metadata": {
        "id": "2ee9a618-d423-4cce-8e32-ed937698c0d5"
      },
      "source": [
        "### Python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e052148c-1dd2-4cd2-8878-fd90e58dabfe",
      "metadata": {
        "id": "e052148c-1dd2-4cd2-8878-fd90e58dabfe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# loading the API Keys from .env\n",
        "load_dotenv(find_dotenv(), override=True)\n",
        "\n",
        "print(os.environ.get('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "654afaf9-00bc-4d4d-99ab-907055d4139c",
      "metadata": {
        "id": "654afaf9-00bc-4d4d-99ab-907055d4139c"
      },
      "source": [
        "## Chat Models: GPT-3.5 Turbo and GPT-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2eb526e-4105-4301-b931-f285f258035b",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2eb526e-4105-4301-b931-f285f258035b",
        "outputId": "e2505d03-4c79-41f7-9fcc-fe0f8891cef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantum mechanics is the branch of physics that describes the behavior of particles at the smallest scales, where traditional laws of physics no longer apply and instead, phenomena such as superposition and entanglement occur.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "# invoking the llm (running the prompt)\n",
        "output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-3.5-turbo', temperature=0.1)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b5b306-6905-4c58-82ef-17cc9be9777d",
      "metadata": {
        "scrolled": true,
        "id": "54b5b306-6905-4c58-82ef-17cc9be9777d"
      },
      "outputs": [],
      "source": [
        "# help(ChatOpenAI)  # see the llm constructor arguments with its defaults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da2baa8e-8440-4c16-a1bb-91f8ff0ac556",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da2baa8e-8440-4c16-a1bb-91f8ff0ac556",
        "outputId": "44225b45-063b-46dd-e854-7a1883ed4a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "క్వాంటం యంత్రశాస్త్రం ఒక శాస్త్రం యొక్క వ్యవహారాత్మక సిద్ధాంతాన్ని వివరిస్తుంది, వాంతి విజ్ఞాన నియమాలను మీరు వివరిస్తున్నారు.\n"
          ]
        }
      ],
      "source": [
        "# using Chat Completions API Messages: System, Assistant and Human\n",
        "from langchain.schema import(\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        "    HumanMessage\n",
        ")\n",
        "messages = [\n",
        "    SystemMessage(content='You are a physicist and respond only in Telugu.'),\n",
        "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
        "]\n",
        "\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919e837c-d307-4165-ad99-551a2bee6cf5",
      "metadata": {
        "id": "919e837c-d307-4165-ad99-551a2bee6cf5"
      },
      "source": [
        "## Caching LLM Responses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7379171e-2081-4cc1-a834-89ebff34e84c",
      "metadata": {
        "id": "7379171e-2081-4cc1-a834-89ebff34e84c"
      },
      "source": [
        "### 1. In-Memory Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a73a4e4-d351-4f4c-b149-151fe3b2c878",
      "metadata": {
        "id": "7a73a4e4-d351-4f4c-b149-151fe3b2c878"
      },
      "outputs": [],
      "source": [
        "from langchain.globals import set_llm_cache\n",
        "from langchain_openai import OpenAI\n",
        "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d356c163-afd3-4534-9947-69e088f16913",
      "metadata": {
        "id": "d356c163-afd3-4534-9947-69e088f16913"
      },
      "outputs": [],
      "source": [
        "from langchain.cache import InMemoryCache\n",
        "set_llm_cache(InMemoryCache())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20232db3-97a5-498e-805f-fbcb24d14584",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "20232db3-97a5-498e-805f-fbcb24d14584",
        "outputId": "03b077fb-6184-4d7a-dd35-a2f10df762b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 30.4 ms, sys: 4.72 ms, total: 35.1 ms\n",
            "Wall time: 765 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nWhy was the math book sad?\\n\\nBecause it had too many problems.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'Tell a me a joke that a toddler can understand.'\n",
        "llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d58e8e-c8b3-4719-b4ad-a3d03bfb7655",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "18d58e8e-c8b3-4719-b4ad-a3d03bfb7655",
        "outputId": "55aafb50-e011-4822-f412-77227e9e24ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 582 µs, sys: 0 ns, total: 582 µs\n",
            "Wall time: 590 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nWhy was the math book sad?\\n\\nBecause it had too many problems.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "%%time\n",
        "llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6269939d-b92f-406a-a53f-9a7bf561c5a2",
      "metadata": {
        "id": "6269939d-b92f-406a-a53f-9a7bf561c5a2"
      },
      "source": [
        "### 2. SQLite Caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9271fe21-1ca3-4cb2-8599-9a368071482d",
      "metadata": {
        "id": "9271fe21-1ca3-4cb2-8599-9a368071482d"
      },
      "outputs": [],
      "source": [
        "from langchain.cache import SQLiteCache\n",
        "set_llm_cache(SQLiteCache(database_path=\"/content/.langchain.db\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a13f4281-b6c7-4a46-ab08-b02528f4111e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "a13f4281-b6c7-4a46-ab08-b02528f4111e",
        "outputId": "c93f33dd-c81e-4ca3-9c91-5b6af084b4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 34.6 ms, sys: 2.99 ms, total: 37.6 ms\n",
            "Wall time: 586 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nWhy was the math book sad?\\n\\nBecause it had too many problems.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "%%time\n",
        "# First request (not in cache, takes longer)\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3fe8a61-0812-427b-bdae-06adc67988f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "e3fe8a61-0812-427b-bdae-06adc67988f6",
        "outputId": "79858476-8f91-4c45-c8b2-d2ca55e88ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17.6 ms, sys: 4.94 ms, total: 22.6 ms\n",
            "Wall time: 26.2 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nWhy was the math book sad?\\n\\nBecause it had too many problems.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "%%time\n",
        "# Second request (cached, faster)\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f56aa044-5842-4d50-b3e9-0207421df1cf",
      "metadata": {
        "id": "f56aa044-5842-4d50-b3e9-0207421df1cf"
      },
      "source": [
        "## LLM Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "174adda0-ce4a-40bb-8a28-31abc494f266",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "174adda0-ce4a-40bb-8a28-31abc494f266",
        "outputId": "4a8f9e5b-1016-4be0-ffbb-fb7d56d313c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verse 1:\n",
            "In the dead of night, under the pale moonlight\n",
            "A raven flies, its wings so black and bright\n",
            "It calls out to the moon, a haunting melody\n",
            "A creature of the night, wild and free\n",
            "\n",
            "Chorus:\n",
            "Moon and raven, dancing in the sky\n",
            "Their spirits intertwined, never asking why\n",
            "They roam the night, together as one\n",
            "Moon and raven, under the midnight sun\n",
            "\n",
            "Verse 2:\n",
            "The moon shines down, a beacon in the dark\n",
            "Guiding the raven on its journey, leaving its mark\n",
            "Through forests and valleys, over mountains high\n",
            "The moon and raven, never say goodbye\n",
            "\n",
            "Chorus:\n",
            "Moon and raven, dancing in the sky\n",
            "Their spirits intertwined, never asking why\n",
            "They roam the night, together as one\n",
            "Moon and raven, under the midnight sun\n",
            "\n",
            "Bridge:\n",
            "The moon whispers secrets, the raven knows them all\n",
            "They share a bond, unbreakable and tall\n",
            "Through the storms and the shadows, they will always be\n",
            "Moon and raven, forever wild and free\n",
            "\n",
            "Chorus:\n",
            "Moon and raven, dancing in the sky\n",
            "Their spirits intertwined, never asking why\n",
            "They roam the night, together as one\n",
            "Moon and raven, under the midnight sun\n",
            "\n",
            "Outro:\n",
            "So if you see a raven, flying high and free\n",
            "Remember the moon, shining down for all to see\n",
            "Their bond is unbreakable, their spirit never dies\n",
            "Moon and raven, forever in the night skies.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "prompt = 'Write a rock song about the Moon and a Raven.'\n",
        "print(llm.invoke(prompt).content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(prompt))"
      ],
      "metadata": {
        "id": "lhvn3SLiJLh0",
        "outputId": "85beda50-5b08-4f87-c3b6-ddb4edc24b26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lhvn3SLiJLh0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Verse 1:\\nIn the dead of night, under the pale moonlight\\nA raven flies, its wings so black and bright\\nIt calls out to the moon, a haunting melody\\nA creature of the night, wild and free\\n\\nChorus:\\nMoon and raven, dancing in the sky\\nTheir spirits intertwined, never asking why\\nThey roam the night, together as one\\nMoon and raven, under the midnight sun\\n\\nVerse 2:\\nThe moon shines down, a beacon in the dark\\nGuiding the raven on its journey, leaving its mark\\nThrough forests and valleys, over mountains high\\nThe moon and raven, never say goodbye\\n\\nChorus:\\nMoon and raven, dancing in the sky\\nTheir spirits intertwined, never asking why\\nThey roam the night, together as one\\nMoon and raven, under the midnight sun\\n\\nBridge:\\nThe moon whispers secrets, the raven knows them all\\nThey share a bond, unbreakable and tall\\nThrough the storms and the shadows, they will always be\\nMoon and raven, forever wild and free\\n\\nChorus:\\nMoon and raven, dancing in the sky\\nTheir spirits intertwined, never asking why\\nThey roam the night, together as one\\nMoon and raven, under the midnight sun\\n\\nOutro:\\nSo if you see a raven, flying high and free\\nRemember the moon, shining down for all to see\\nTheir bond is unbreakable, their spirit never dies\\nMoon and raven, forever in the night skies.' id='run-60a926e8-faf6-4768-a26f-99372d4819f4-0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = \"Its piercing eyes, a story yet untold,\"\n",
        "print(c,end='.')"
      ],
      "metadata": {
        "id": "zSjjt9IxJSYL",
        "outputId": "b2933cb8-b833-48ba-b6eb-eff0bdc81054",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zSjjt9IxJSYL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Its piercing eyes, a story yet untold,."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a94d50a4-60ca-4009-a051-b83cf70c7a45",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a94d50a4-60ca-4009-a051-b83cf70c7a45",
        "outputId": "9b49c5a5-071a-4561-d3e4-b5dcb2d97070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verse 1:\n",
            "Beneath the midnight sky\n",
            "The moon shines bright up high\n",
            "A raven flies by\n",
            "Its silhouette against the night\n",
            "\n",
            "Chorus:\n",
            "Oh, moon and raven\n",
            "Dancing in the dark\n",
            "A mysterious connection\n",
            "Leaving its mark\n",
            "\n",
            "Verse 2:\n",
            "The raven calls out\n",
            "A haunting melody\n",
            "As the moon whispers secrets\n",
            "To the creatures of the night\n",
            "\n",
            "Chorus:\n",
            "Oh, moon and raven\n",
            "Guiding us through the night\n",
            "A symbol of mystery\n",
            "In the pale moonlight\n",
            "\n",
            "Bridge:\n",
            "The moon's silver glow\n",
            "Reflecting in the raven's eye\n",
            "A bond between two worlds\n",
            "That will never die\n",
            "\n",
            "Chorus:\n",
            "Oh, moon and raven\n",
            "Forever intertwined\n",
            "In the endless cycle\n",
            "Of the night's design\n",
            "\n",
            "Outro:\n",
            "As the raven takes flight\n",
            "And the moon begins to fade\n",
            "Their connection remains\n",
            "In the darkness they've made."
          ]
        }
      ],
      "source": [
        "for chunk in llm.stream(prompt):\n",
        "    print(chunk.content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84844cfe-7469-47a6-841c-0fc31d8cdb49",
      "metadata": {
        "id": "84844cfe-7469-47a6-841c-0fc31d8cdb49"
      },
      "source": [
        "## PromptTemplates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34389e61-d08b-440a-8a6f-96e99b5bee8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "34389e61-d08b-440a-8a6f-96e99b5bee8c",
        "outputId": "ea1a27fd-dbea-4d26-892e-c5eaea51a414"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an experienced virologist.\\nWrite a few sentences about the following virus \"hiv\" in telugu.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Define a template for the prompt\n",
        "template = '''You are an experienced virologist.\n",
        "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
        "\n",
        "# Create a PromptTemplate object from the template\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "# Fill in the variable: virus and language\n",
        "prompt = prompt_template.format(virus='hiv', language='telugu')\n",
        "prompt  # Returns the generated prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "602e2f25-4ba8-4a48-b116-4589af56f9fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "602e2f25-4ba8-4a48-b116-4589af56f9fc",
        "outputId": "3b7d5a4c-c36e-45da-b333-1ea82b250a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HIV వైరస్ ఒక మానవ ఇమ్యూనోడీఫీషీన్సీ వైరస్ (HIV) మరియు ఏడు విధానాలు ఉంటాయి. ఇది మానవ శరీరంలో కనిపించే మరియు మన ఇమ్యూనోసిస్టం నిలిచిపోవడానికి కారణం ఉంటుంది. ఈ వైరస్ మానవ జీవితాన్ని ప్రభావితం చేస్తుంది మరియు ఏడు విధానాలు ఉంటాయి.\n"
          ]
        }
      ],
      "source": [
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
        "output = llm.invoke(prompt)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8084261-b4c6-47e6-8a38-03f339f1f5c5",
      "metadata": {
        "id": "d8084261-b4c6-47e6-8a38-03f339f1f5c5"
      },
      "source": [
        "## ChatPromptTemplates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fab3930-930d-4c45-a3d8-a6b213d77270",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fab3930-930d-4c45-a3d8-a6b213d77270",
        "outputId": "99230b9c-098c-4334-b920-96bc6bd3b412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You respond only in the JSON format.'), HumanMessage(content='Top 5 countries in World by population.')]\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# Create a chat template with system and human messages\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content='You respond only in the JSON format.'),\n",
        "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fill in the specific values for n and area\n",
        "messages = chat_template.format_messages(n='5', area='World')\n",
        "print(messages)  # Outputs the formatted chat messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a768271-ff8d-4d4c-a7c3-21ebed1abd6a",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a768271-ff8d-4d4c-a7c3-21ebed1abd6a",
        "outputId": "7f3a702e-cc51-44bd-acca-81f121e05cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"countries\": [\n",
            "        {\n",
            "            \"rank\": 1,\n",
            "            \"name\": \"China\",\n",
            "            \"population\": \"1,439,323,776\"\n",
            "        },\n",
            "        {\n",
            "            \"rank\": 2,\n",
            "            \"name\": \"India\",\n",
            "            \"population\": \"1,380,004,385\"\n",
            "        },\n",
            "        {\n",
            "            \"rank\": 3,\n",
            "            \"name\": \"United States\",\n",
            "            \"population\": \"331,002,651\"\n",
            "        },\n",
            "        {\n",
            "            \"rank\": 4,\n",
            "            \"name\": \"Indonesia\",\n",
            "            \"population\": \"273,523,615\"\n",
            "        },\n",
            "        {\n",
            "            \"rank\": 5,\n",
            "            \"name\": \"Pakistan\",\n",
            "            \"population\": \"220,892,340\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI()\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb2eb160-df97-4229-8115-2e8864b400d3",
      "metadata": {
        "id": "eb2eb160-df97-4229-8115-2e8864b400d3"
      },
      "source": [
        "## Simple Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2aef22-26c3-4e8c-8bbd-99383b6052e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e2aef22-26c3-4e8c-8bbd-99383b6052e9",
        "outputId": "c1780342-7cc7-49e7-951d-2fb1334e1a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an experience virologist.\n",
            "Write a few sentences about the following virus \"HSV\" in Spanish.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "template = '''You are an experience virologist.\n",
        "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "output = chain.invoke({'virus': 'HSV', 'language': 'Spanish'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161f079d-a38d-48f5-8466-9b2c185446f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "161f079d-a38d-48f5-8466-9b2c185446f4",
        "outputId": "da5ff742-78ca-47a7-bc98-fcb9ff2829ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'virus': 'HSV', 'language': 'Spanish', 'text': 'El virus del herpes simple (HSV) es un virus de doble cadena de ADN que pertenece a la familia Herpesviridae. Hay dos tipos principales de HSV: HSV-1, que generalmente causa herpes labial en la boca, y HSV-2, que generalmente causa herpes genital. Ambos tipos de virus son altamente contagiosos y pueden causar brotes recurrentes de lesiones en la piel y membranas mucosas. Actualmente no existe una cura para el HSV, pero los medicamentos antivirales pueden ayudar a reducir la frecuencia y gravedad de los brotes.'}\n"
          ]
        }
      ],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c23cc0a0-6b02-4007-9192-80f00d7576a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c23cc0a0-6b02-4007-9192-80f00d7576a4",
        "outputId": "99348142-a47c-4957-d971-3b069f2e7f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Country: india\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWhat is the capital of india?. List the top 3 places to visit in that city. Use bullet points\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The capital of India is New Delhi.\n",
            "\n",
            "Top 3 places to visit in New Delhi:\n",
            "- Red Fort\n",
            "- India Gate\n",
            "- Qutub Minar\n"
          ]
        }
      ],
      "source": [
        "template = 'What is the capital of {country}?. List the top 3 places to visit in that city. Use bullet points'\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "# Initialize an LLMChain with the ChatOpenAI model and the prompt template\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "country = input('Enter Country: ')\n",
        "\n",
        "# Invoke the chain with specific virus and language values\n",
        "output = chain.invoke(country)\n",
        "print(output['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "963536b8-76e7-4681-bb0d-8309795e2fa3",
      "metadata": {
        "id": "963536b8-76e7-4681-bb0d-8309795e2fa3"
      },
      "source": [
        "## Sequential Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e711bc-8bcf-45ce-b993-52e42fb650cd",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9e711bc-8bcf-45ce-b993-52e42fb650cd",
        "outputId": "8ce1494b-fd76-408d-f597-bf8722c7412a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mSure! Here is a Python function that implements linear regression:\n",
            "\n",
            "```python\n",
            "import numpy as np\n",
            "\n",
            "def linear_regression(x, y):\n",
            "    n = len(x)\n",
            "    x_mean = np.mean(x)\n",
            "    y_mean = np.mean(y)\n",
            "\n",
            "    numerator = 0\n",
            "    denominator = 0\n",
            "    for i in range(n):\n",
            "        numerator += (x[i] - x_mean) * (y[i] - y_mean)\n",
            "        denominator += (x[i] - x_mean) ** 2\n",
            "\n",
            "    slope = numerator / denominator\n",
            "    intercept = y_mean - slope * x_mean\n",
            "\n",
            "    return slope, intercept\n",
            "\n",
            "# Example usage\n",
            "x = np.array([1, 2, 3, 4, 5])\n",
            "y = np.array([2, 4, 5, 4, 5])\n",
            "\n",
            "slope, intercept = linear_regression(x, y)\n",
            "print(f\"Slope: {slope}, Intercept: {intercept}\")\n",
            "```\n",
            "\n",
            "This function takes two arrays `x` and `y` as input, where `x` is the independent variable and `y` is the dependent variable. It calculates the slope and intercept of the linear regression line that best fits the data points `(x, y)`. The function then returns the slope and intercept values.\n",
            "\n",
            "You can use this function with your own dataset by passing in your `x` and `y` arrays.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mSure! This Python function `linear_regression` implements the calculation of the slope and intercept for a simple linear regression model. Here is a step-by-step explanation of how the function works:\n",
            "\n",
            "1. The function `linear_regression` takes two input parameters `x` and `y`, which are assumed to be arrays representing the independent and dependent variables respectively.\n",
            "\n",
            "2. It calculates the number of observations in the input arrays by getting the length of one of the arrays using `len(x)`.\n",
            "\n",
            "3. It calculates the mean values of the `x` and `y` arrays using NumPy's `np.mean()` function, storing them in the variables `x_mean` and `y_mean` respectively.\n",
            "\n",
            "4. It initializes `numerator` and `denominator` variables to 0, which will be used to calculate the slope later.\n",
            "\n",
            "5. It then enters a loop that iterates over each observation in the input arrays using the range function and calculates the numerator and denominator needed for the calculation of the slope using the formula for the correlation coefficient.\n",
            "\n",
            "6. After the loop completes, the function calculates the slope by dividing the numerator by the denominator, as per the formula used in linear regression.\n",
            "\n",
            "7. It calculates the intercept of the linear regression line using the formula `(y_mean - slope * x_mean)`.\n",
            "\n",
            "8. Finally, it returns the slope and intercept values as a tuple `(slope, intercept)`.\n",
            "\n",
            "9. In the example usage provided at the end of the code snippet, it creates sample arrays `x` and `y`, calls the `linear_regression` function with those arrays, and then prints out the calculated slope and intercept values.\n",
            "\n",
            "You can use this function with your own dataset by passing in your `x` and `y` arrays that represent your independent and dependent variables, respectively. The function will then provide you with the slope and intercept values that best fit a linear regression model to your data points.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "\n",
        "# Initialize the first ChatOpenAI model (gpt-3.5-turbo) with specific temperature\n",
        "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
        "\n",
        "# Define the first prompt template\n",
        "prompt_template1 = PromptTemplate.from_template(\n",
        "    template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
        ")\n",
        "# Create an LLMChain using the first model and the prompt template\n",
        "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
        "\n",
        "# Initialize the second ChatOpenAI model (gpt-4-turbo) with specific temperature\n",
        "llm2 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
        "\n",
        "# Define the second prompt template\n",
        "prompt_template2 = PromptTemplate.from_template(\n",
        "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
        ")\n",
        "# Create another LLMChain using the second model and the prompt template\n",
        "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
        "\n",
        "# Combine both chains into a SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
        "\n",
        "# Invoke the overall chain with the concept \"linear regression\"\n",
        "output = overall_chain.invoke('linear regression')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b365ca5e-84f6-484f-829b-1ba9ff9798e0",
      "metadata": {
        "id": "b365ca5e-84f6-484f-829b-1ba9ff9798e0",
        "outputId": "108be063-2bb8-4412-fa8f-56047946851a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Python function `linear_regression` is an implementation of simple linear regression, which is a fundamental statistical approach used to model the relationship between a single independent variable (denoted as `x`) and a dependent variable (denoted as `y`). The main goal of this function is to find the best-fitting straight line (also known as the regression line) that describes this relationship. The equation of this line is `y = beta1 * x + beta0`, where `beta1` is the slope of the line, and `beta0` is the y-intercept.\n",
            "\n",
            "The implementation details of the `linear_regression` function are explained below:\n",
            "\n",
            "1. **Imports and Dependencies**: The function uses NumPy (imported as `np`), a popular library for numerical computing in Python. NumPy provides efficient ways to handle arrays and mathematical operations, making it an ideal choice for implementing linear regression.\n",
            "\n",
            "2. **Parameters**: The function accepts two parameters, `x` and `y`, which are NumPy arrays of the same length. The elements of `x` represent the independent variable values, and the elements of `y` represent the dependent variable values.\n",
            "\n",
            "3. **Number of Data Points**: The function starts by determining `n`, the number of data points, which is the length of the `x` array (and equivalently, the `y` array).\n",
            "\n",
            "4. **Mean Calculation**: It calculates the mean (average) values of both `x` and `y` using `np.mean()`. These mean values are crucial for determining the slope and intercept of the regression line.\n",
            "\n",
            "5. **Slope (Beta1) and Intercept (Beta0) Calculation**: \n",
            "    - The slope (`beta1`) of the regression line measures the change in the dependent variable (`y`) for a one-unit change in the independent variable (`x`). It is calculated by dividing the sum of the products of differences between each `x` value and the mean of `x` with corresponding differences between each `y` value and the mean of `y`, by the sum of the squares of differences between each `x` value and the mean of `x`. Mathematically, it is expressed as: \n",
            "    \\[ beta1 = \\frac{\\sum ((x_i - x_{mean}) * (y_i - y_{mean}))}{\\sum ((x_i - x_{mean})^2)} \\]\n",
            "    \n",
            "    - The intercept (`beta0`) of the regression line represents the value of `y` when `x` is zero. It is calculated using the relationship between the means of `x` and `y`, and the slope (`beta1`). Mathematically: \n",
            "    \\[ beta0 = y_{mean} - beta1 * x_{mean} \\]\n",
            "\n",
            "6. **Return Values**: Finally, the function returns the calculated values of the slope (`beta1`) and intercept (`beta0`) of the regression line. \n",
            "\n",
            "To use this function, the user needs to provide arrays of `x` and `y` values, and the function will output the slope and intercept of the line that fits best to these points in the context of least squares fitting. These outputs essentially define the linear equation that best describes the relationship between the independent and dependent variables in the given dataset. The given implementation is a clear example for teaching or understanding the basics of simple linear regression without relying on higher-level APIs or libraries that obfuscate these fundamental calculations.\n"
          ]
        }
      ],
      "source": [
        "print(output['output'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a8cee1-18fa-4068-a234-defdb7ed2b2e",
      "metadata": {
        "id": "d2a8cee1-18fa-4068-a234-defdb7ed2b2e"
      },
      "outputs": [],
      "source": [
        "pip install -q langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c9b9413-9d79-47e4-8464-40cb3756700e",
      "metadata": {
        "id": "6c9b9413-9d79-47e4-8464-40cb3756700e",
        "outputId": "95bdcb95-4a1a-44bb-9abf-51ec51b56ffe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'[13, 26, 39, 52, 65, 78, 91]\\n'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_experimental.utilities import PythonREPL\n",
        "python_repl = PythonREPL()\n",
        "python_repl.run('print([n for n in range(1, 100) if n % 13 == 0])')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9973644c-65a3-44ac-ac9e-1be39950acdd",
      "metadata": {
        "id": "9973644c-65a3-44ac-ac9e-1be39950acdd",
        "outputId": "a03aecd8-7574-4ad7-ed7d-dfbdad485856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo solve this, I need to calculate the factorial of 12 first, then find its square root, and finally format the result to display it with 4 decimal points. I can use the `math` module in Python for both the factorial and square root calculations.\n",
            "Action: Python_REPL\n",
            "Action Input: import math\n",
            "print(f\"{math.sqrt(math.factorial(12)):.4f}\")\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m21886.1052\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 21886.1052\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Calculate the square root of the factorial of 12 and display it with 4 decimal points',\n",
              " 'output': '21886.1052'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the ChatOpenAI model with gpt-4-turbo and a temperature of 0\n",
        "llm = ChatOpenAI(model='gpt-4-turbo-preview', temperature=0)\n",
        "\n",
        "# Create a Python agent using the ChatOpenAI model and a PythonREPLTool\n",
        "agent_executor = create_python_agent(\n",
        "    llm=llm,\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Invoke the agent\n",
        "prompt = 'Calculate the square root of the factorial of 12 and display it with 4 decimal points'\n",
        "agent_executor.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a1046e3-42dd-41a1-be25-8f48454f92f6",
      "metadata": {
        "id": "8a1046e3-42dd-41a1-be25-8f48454f92f6",
        "outputId": "07e8b5ee-d183-405f-e1c7-9dbd30425c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to calculate 5.1 raised to the power of 7.3 to get the answer.\n",
            "Action: Python_REPL\n",
            "Action Input: print(5.1 ** 7.3)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 146306.05007233328\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "response = agent_executor.invoke('What is the answer to 5.1 ** 7.3?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af92f7d0-06c4-475e-9deb-8ab0352050ab",
      "metadata": {
        "id": "af92f7d0-06c4-475e-9deb-8ab0352050ab",
        "outputId": "3869dba1-0dcc-4008-c597-1299927f8772"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'What is the answer to 5.1 ** 7.3?', 'output': '146306.05007233328'}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e936c363-5720-443b-b4f9-29d0b5263536",
      "metadata": {
        "id": "e936c363-5720-443b-b4f9-29d0b5263536",
        "outputId": "e8f22192-b59a-4d4b-efc8-a14e4409bc20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the answer to 5.1 ** 7.3?\n"
          ]
        }
      ],
      "source": [
        "print(response['input'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1accfc09-b6c0-4c0a-8cc6-54590d081f86",
      "metadata": {
        "id": "1accfc09-b6c0-4c0a-8cc6-54590d081f86",
        "outputId": "c915776d-5664-485b-a224-9c1b71fa043b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "146306.05007233328\n"
          ]
        }
      ],
      "source": [
        "print(response['output'])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}