{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5f10b4d1",
      "metadata": {
        "id": "5f10b4d1"
      },
      "source": [
        "# Splitting and Embedding Text Using LangChain (Similarity Search)\n",
        "\n",
        "This notebook uses the latest versions of the libraries OpenAI, LangChain, and Pinecone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b68c5a",
      "metadata": {
        "id": "25b68c5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f6b673-91a4-4814-f2d2-ee8fb040a82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m215.9/215.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.0/116.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m743.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "  !pip install -q -r ./requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81676843",
      "metadata": {
        "id": "81676843"
      },
      "source": [
        "Download [requirements.txt](https://drive.google.com/file/d/1UpURYL9kqjXfe9J8o-_Dq5KJTbQpzMef/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8859749e",
      "metadata": {
        "id": "8859749e",
        "outputId": "87979692-ef71-4631-cc41-2fbe33ce560f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cedb18d",
      "metadata": {
        "id": "5cedb18d"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "with open('/content/churchill_speech.txt') as f:\n",
        "    churchill_speech = f.read()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0693ae6",
      "metadata": {
        "id": "e0693ae6",
        "outputId": "db5f1dd9-d2e7-4bf3-d803-41e38fc02755",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now you have 300\n"
          ]
        }
      ],
      "source": [
        "chunks = text_splitter.create_documents([churchill_speech])\n",
        "# print(chunks[2])\n",
        "# print(chunks[10].page_content)\n",
        "print(f'Now you have {len(chunks)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df4d686b",
      "metadata": {
        "id": "df4d686b"
      },
      "source": [
        "#### Embedding Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ed819e6",
      "metadata": {
        "id": "8ed819e6",
        "outputId": "14b05b2e-0992-4332-969b-ac749de35ea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 4820\n",
            "Embedding Cost in USD: 0.000096\n"
          ]
        }
      ],
      "source": [
        "def print_embedding_cost(texts):\n",
        "    import tiktoken\n",
        "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
        "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
        "    # check prices here: https://openai.com/pricing\n",
        "    print(f'Total Tokens: {total_tokens}')\n",
        "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')\n",
        "\n",
        "print_embedding_cost(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdf79b3b",
      "metadata": {
        "id": "bdf79b3b"
      },
      "source": [
        "### Creating embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53572e13",
      "metadata": {
        "id": "53572e13"
      },
      "outputs": [],
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings('ignore', module='langchain')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bdfd40d",
      "metadata": {
        "id": "3bdfd40d"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "425c3c35",
      "metadata": {
        "id": "425c3c35"
      },
      "outputs": [],
      "source": [
        "vector = embeddings.embed_query(chunks[0].page_content)\n",
        "vector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96f866ff",
      "metadata": {
        "id": "96f866ff"
      },
      "source": [
        "### Inserting the Embeddings into a Pinecone Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5215b7e7",
      "metadata": {
        "id": "5215b7e7"
      },
      "outputs": [],
      "source": [
        "# I'm importing the necessary libraries and initializing the Pinecone client\n",
        "import os\n",
        "import pinecone\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "\n",
        "pc = pinecone.Pinecone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3b94b4",
      "metadata": {
        "id": "bb3b94b4",
        "outputId": "4a1282f9-6f4f-4450-f199-0bae3860433e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting all indexes ... Done\n"
          ]
        }
      ],
      "source": [
        "# deleting all indexes\n",
        "indexes = pc.list_indexes().names()\n",
        "for i in indexes:\n",
        "    print('Deleting all indexes ... ', end='')\n",
        "    pc.delete_index(i)\n",
        "    print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9404a9",
      "metadata": {
        "id": "cc9404a9",
        "outputId": "9973ec79-38d0-4036-caf9-0d238e872ee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating index churchill-speech\n",
            "Index created! ğŸ˜Š\n"
          ]
        }
      ],
      "source": [
        "# creating an index\n",
        "from pinecone import PodSpec\n",
        "index_name = 'churchill-speech'\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    print(f'Creating index {index_name}')\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric='cosine',\n",
        "        spec=PodSpec(\n",
        "            environment='gcp-starter'\n",
        "        )\n",
        "    )\n",
        "    print('Index created! ğŸ˜Š')\n",
        "else:\n",
        "    print(f'Index {index_name} already exists!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'churchill-speech'"
      ],
      "metadata": {
        "id": "VM_ew7qIpgU0"
      },
      "id": "VM_ew7qIpgU0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb84fba",
      "metadata": {
        "id": "9bb84fba"
      },
      "outputs": [],
      "source": [
        "# processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
        "# inserting the embeddings into the index and returning a new Pinecone vector store object.\n",
        "vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "014acab8-ecd0-456c-976d-4b5890f8b0ab",
      "metadata": {
        "id": "014acab8-ecd0-456c-976d-4b5890f8b0ab",
        "outputId": "63d91382-4e39-43ca-d0eb-ed5a912150d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.003,\n",
              " 'namespaces': {'': {'vector_count': 300}},\n",
              " 'total_vector_count': 300}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb0b01d",
      "metadata": {
        "id": "0cb0b01d"
      },
      "source": [
        "### Asking Questions (Similarity Search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4cccccd",
      "metadata": {
        "id": "e4cccccd",
        "outputId": "cfc26b87-464b-4ecb-edd7-0ad3e53bf781",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields and'), Document(page_content='end, we shall fight in France, we shall fight on the seas and oceans, we shall fight with growing'), Document(page_content='streets, we shall fight in the hills; we shall never surrender, and even if, which I do not for a'), Document(page_content='number of the enemy, and fought fiercely on some of the old grounds that so many of us knew so')]\n"
          ]
        }
      ],
      "source": [
        "query = 'Where should we fight?'\n",
        "result = vector_store.similarity_search(query)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bdacf56",
      "metadata": {
        "scrolled": true,
        "id": "0bdacf56",
        "outputId": "d03e60e8-e7c0-4710-c428-9cee0743024a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields and\n",
            "--------------------------------------------------\n",
            "end, we shall fight in France, we shall fight on the seas and oceans, we shall fight with growing\n",
            "--------------------------------------------------\n",
            "streets, we shall fight in the hills; we shall never surrender, and even if, which I do not for a\n",
            "--------------------------------------------------\n",
            "number of the enemy, and fought fiercely on some of the old grounds that so many of us knew so\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for r in result:\n",
        "    print(r.page_content)\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f155a46",
      "metadata": {
        "id": "5f155a46"
      },
      "source": [
        "### Answering in Natural Language using an LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19cf0082",
      "metadata": {
        "id": "19cf0082"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the LLM with the specified model and temperature\n",
        "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.2)\n",
        "\n",
        "# Use the provided vector store with similarity search and retrieve top 3 results\n",
        "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
        "\n",
        "# Create a RetrievalQA chain using the defined LLM, chain type 'stuff', and retriever\n",
        "chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf8e6835",
      "metadata": {
        "id": "bf8e6835",
        "outputId": "af25133c-6fff-4443-96f8-6ef4cf586891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'Answer only from the provided input. Where should we fight?', 'result': 'We shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields and end, we shall fight in France, we shall fight on the seas and oceans.'}\n"
          ]
        }
      ],
      "source": [
        "query = 'Answer only from the provided input. Where should we fight?'\n",
        "answer = chain.invoke(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09c9ab37",
      "metadata": {
        "id": "09c9ab37",
        "outputId": "970985e2-b5d1-482a-ad6e-051257ec1278",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The King of Belgium at that time was King Leopold.\n"
          ]
        }
      ],
      "source": [
        "query = 'Who was the king of Belgium at that time?'\n",
        "answer = chain.run(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8688ba8",
      "metadata": {
        "id": "c8688ba8",
        "outputId": "46bf8c3c-597c-4ffd-d8fa-c6d5011a7812",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The French Armies were involved in the fighting alongside the British during the conflict.\n"
          ]
        }
      ],
      "source": [
        "query = 'What about the French Armies??'\n",
        "answer = chain.run(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2431925c-b2ba-4c10-9e25-48daad395a31",
      "metadata": {
        "id": "2431925c-b2ba-4c10-9e25-48daad395a31"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}