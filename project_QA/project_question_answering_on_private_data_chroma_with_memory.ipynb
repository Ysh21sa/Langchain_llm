{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "640e4bbc",
      "metadata": {
        "id": "640e4bbc"
      },
      "source": [
        "# Project: Question-Answering on Private Documents (RAG)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r ./requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGRmfCV4PEbe",
        "outputId": "b1dd584d-7c50-44bb-a919-95ae1e618877"
      },
      "id": "FGRmfCV4PEbe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.5/214.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b748fd20",
      "metadata": {
        "id": "b748fd20",
        "outputId": "e8cf21ad-bef0-4222-ee3b-1497418aad0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f13727cc",
      "metadata": {
        "id": "f13727cc"
      },
      "outputs": [],
      "source": [
        "!pip install -q pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4bdf053",
      "metadata": {
        "id": "f4bdf053"
      },
      "outputs": [],
      "source": [
        "!pip install -q docx2txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111f6b22",
      "metadata": {
        "id": "111f6b22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13a2322-b867-4dfb-a487-faeb9289f8e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2732726-ea0e-4926-a8ae-cb086d856aa9",
      "metadata": {
        "id": "d2732726-ea0e-4926-a8ae-cb086d856aa9"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -q openai langchain tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07900179",
      "metadata": {
        "id": "07900179"
      },
      "source": [
        "## Loading Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1ae19b5",
      "metadata": {
        "id": "e1ae19b5"
      },
      "outputs": [],
      "source": [
        "# loading PDF, DOCX and TXT files as LangChain Documents\n",
        "def load_document(file):\n",
        "    import os\n",
        "    name, extension = os.path.splitext(file)\n",
        "\n",
        "    if extension == '.pdf':\n",
        "        from langchain.document_loaders import PyPDFLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = PyPDFLoader(file)\n",
        "    elif extension == '.docx':\n",
        "        from langchain.document_loaders import Docx2txtLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = Docx2txtLoader(file)\n",
        "    elif extension == '.txt':\n",
        "        from langchain.document_loaders import TextLoader\n",
        "        loader = TextLoader(file)\n",
        "    else:\n",
        "        print('Document format is not supported!')\n",
        "        return None\n",
        "\n",
        "    data = loader.load()\n",
        "    return data\n",
        "\n",
        "\n",
        "# wikipedia\n",
        "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
        "    from langchain.document_loaders import WikipediaLoader\n",
        "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
        "    data = loader.load()\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "367bb8ad",
      "metadata": {
        "id": "367bb8ad"
      },
      "source": [
        "## Chunking Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62c3cb39",
      "metadata": {
        "id": "62c3cb39"
      },
      "outputs": [],
      "source": [
        "def chunk_data(data, chunk_size=256):\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
        "    chunks = text_splitter.split_documents(data)\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b298177",
      "metadata": {
        "id": "2b298177"
      },
      "source": [
        "## Calculating Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add191c6",
      "metadata": {
        "id": "add191c6"
      },
      "outputs": [],
      "source": [
        "def print_embedding_cost(texts):\n",
        "    import tiktoken\n",
        "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
        "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
        "    print(f'Total Tokens: {total_tokens}')\n",
        "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27f92afa",
      "metadata": {
        "id": "27f92afa"
      },
      "source": [
        "## Embedding and Uploading to a Vector Database (Pinecone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c72c3cf",
      "metadata": {
        "id": "0c72c3cf"
      },
      "outputs": [],
      "source": [
        "def insert_or_fetch_embeddings(index_name, chunks):\n",
        "    # importing the necessary libraries and initializing the Pinecone client\n",
        "    import pinecone\n",
        "    from langchain_community.vectorstores import Pinecone\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    from pinecone import PodSpec\n",
        "\n",
        "\n",
        "    pc = pinecone.Pinecone()\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well\n",
        "\n",
        "    # loading from existing index\n",
        "    if index_name in pc.list_indexes():\n",
        "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
        "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        # creating the index and embedding the chunks into the index\n",
        "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
        "\n",
        "        # creating a new index\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1536,\n",
        "            metric='cosine',\n",
        "            spec=PodSpec(\n",
        "                environment='gcp-starter'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
        "        # inserting the embeddings into the index and returning a new Pinecone vector store object.\n",
        "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
        "        print('Ok')\n",
        "\n",
        "    return vector_store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ef1091",
      "metadata": {
        "id": "52ef1091"
      },
      "outputs": [],
      "source": [
        "def delete_pinecone_index(index_name='all'):\n",
        "    import pinecone\n",
        "    pc = pinecone.Pinecone()\n",
        "\n",
        "    if index_name == 'all':\n",
        "        indexes = pc.list_indexes().names()\n",
        "        print('Deleting all indexes ... ')\n",
        "        for index in indexes:\n",
        "            pc.delete_index(index)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        print(f'Deleting index {index_name} ...', end='')\n",
        "        pc.delete_index(index_name)\n",
        "        print('Ok')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "462a94cb",
      "metadata": {
        "id": "462a94cb"
      },
      "source": [
        "## Asking and Getting Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b0a508",
      "metadata": {
        "id": "08b0a508"
      },
      "outputs": [],
      "source": [
        "def ask_and_get_answer(vector_store, q, k=3):\n",
        "    from langchain.chains import RetrievalQA\n",
        "    from langchain_openai import ChatOpenAI\n",
        "\n",
        "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
        "\n",
        "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
        "\n",
        "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "    answer = chain.invoke(q)\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1efe8fc",
      "metadata": {
        "id": "c1efe8fc"
      },
      "source": [
        "## Using Chroma as a Vector DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a67c9969-9a26-4763-a303-45717144eb45",
      "metadata": {
        "id": "a67c9969-9a26-4763-a303-45717144eb45",
        "outputId": "829fb9bf-39c3-4dea-f65c-4a57fc78f900",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01aaa44b-fc99-4021-8307-6091e071a9e3",
      "metadata": {
        "id": "01aaa44b-fc99-4021-8307-6091e071a9e3"
      },
      "outputs": [],
      "source": [
        "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
        "    from langchain.vectorstores import Chroma\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "    # Instantiate an embedding model from OpenAI (smaller version for efficiency)\n",
        "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
        "\n",
        "    # Create a Chroma vector store using the provided text chunks and embedding model,\n",
        "    # configuring it to save data to the specified directory\n",
        "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n",
        "\n",
        "    return vector_store  # Return the created vector store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "432555ac-a13f-4e6f-8e5f-e8b350970a81",
      "metadata": {
        "id": "432555ac-a13f-4e6f-8e5f-e8b350970a81"
      },
      "outputs": [],
      "source": [
        "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
        "    from langchain.vectorstores import Chroma\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "    # Instantiate the same embedding model used during creation\n",
        "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
        "\n",
        "    # Load a Chroma vector store from the specified directory, using the provided embedding function\n",
        "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "    return vector_store  # Return the loaded vector store\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4938e99-ad31-4093-a5e3-e774fcbd6385",
      "metadata": {
        "id": "b4938e99-ad31-4093-a5e3-e774fcbd6385"
      },
      "source": [
        "#### Running Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947584f7-3d74-4562-a4b0-a4588fdc57fe",
      "metadata": {
        "id": "947584f7-3d74-4562-a4b0-a4588fdc57fe",
        "outputId": "64906a99-4e9f-4f75-90d2-96acc353224c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ./rag_powered_by_google_search.pdf\n"
          ]
        }
      ],
      "source": [
        "# Loading the pdf document into LangChain\n",
        "data = load_document('./rag_powered_by_google_search.pdf')\n",
        "\n",
        "# Splitting the document into chunks\n",
        "chunks = chunk_data(data, chunk_size=256)\n",
        "\n",
        "# Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
        "vector_store = create_embeddings_chroma(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2625b802-00f7-47ae-b093-5e55cffdbea9",
      "metadata": {
        "id": "2625b802-00f7-47ae-b093-5e55cffdbea9",
        "outputId": "b18fca8d-eae3-43c4-852b-34e1329667e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'What is Vertex AI Search?', 'result': 'Vertex AI Search is a fully-managed platform that offers customizable answers, search tuning, vector search, grounding, and compliance updates for enterprises. It provides generative AI capabilities and enterprise-ready features for organizations looking to streamline their search technology and focus on building innovative AI applications.'}\n"
          ]
        }
      ],
      "source": [
        "# Asking questions\n",
        "q = 'What is Vertex AI Search?'\n",
        "answer = ask_and_get_answer(vector_store, q)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76d77f8-aaa7-43b4-83cd-c4087afe718f",
      "metadata": {
        "id": "b76d77f8-aaa7-43b4-83cd-c4087afe718f",
        "outputId": "0bb34f6e-4dd1-4e4d-c706-e04e9d55dbc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI Search is a fully-managed platform that offers customizable answers, search tuning, vector search, grounding, and compliance updates for enterprises. It provides generative AI capabilities and enterprise-ready features for organizations looking to streamline their search technology and focus on building innovative AI applications.\n"
          ]
        }
      ],
      "source": [
        "print(answer['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "700b0c4d-a247-4aa4-b7c4-93c3e656c8d7",
      "metadata": {
        "id": "700b0c4d-a247-4aa4-b7c4-93c3e656c8d7",
        "outputId": "7b42fd1e-48ed-4753-b9f1-2b3caa2eaf98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'How many pairs of questions and answers had the StackOverflow dataset?', 'result': \"I don't have the specific number of pairs of questions and answers in the StackOverflow dataset.\"}\n"
          ]
        }
      ],
      "source": [
        "# Load a Chroma vector store from the specified directory (default ./chroma_db)\n",
        "db = load_embeddings_chroma()\n",
        "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
        "answer = ask_and_get_answer(vector_store, q)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ba55a05-d1c4-4034-b883-967f06a70aaa",
      "metadata": {
        "id": "6ba55a05-d1c4-4034-b883-967f06a70aaa",
        "outputId": "8e1067fe-3b4a-465c-a73d-e5b43b1596da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I do not have a specific number to multiply by 2 based on the context provided.\n"
          ]
        }
      ],
      "source": [
        "# We can't ask follow-up questions. There is no memory (chat history) available.\n",
        "q = 'Multiply that number by 2.'\n",
        "answer = ask_and_get_answer(vector_store, q)\n",
        "print(answer['result'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2259f6a4-a2ad-48f2-b414-225b35a49cc5",
      "metadata": {
        "id": "2259f6a4-a2ad-48f2-b414-225b35a49cc5"
      },
      "source": [
        "## Adding Memory (Chat History)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93410211-0cb2-4524-9e90-7d7f67173704",
      "metadata": {
        "id": "93410211-0cb2-4524-9e90-7d7f67173704"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain  # Import class for building conversational AI chains\n",
        "from langchain.memory import ConversationBufferMemory  # Import memory for storing conversation history\n",
        "\n",
        "# Instantiate a ChatGPT LLM (temperature controls randomness)\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
        "\n",
        "# Configure vector store to act as a retriever (finding similar items, returning top 5)\n",
        "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
        "\n",
        "\n",
        "# Create a memory buffer to track the conversation\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "crc = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,  # Link the ChatGPT LLM\n",
        "    retriever=retriever,  # Link the vector store based retriever\n",
        "    memory=memory,  # Link the conversation memory\n",
        "    chain_type='stuff',  # Specify the chain type\n",
        "    verbose=False  # Set to True to enable verbose logging for debugging\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38309528-51c9-4b4a-b8d4-4b26d3163173",
      "metadata": {
        "id": "38309528-51c9-4b4a-b8d4-4b26d3163173"
      },
      "outputs": [],
      "source": [
        "# create a function to ask questions\n",
        "def ask_question(q, chain):\n",
        "    result = chain.invoke({'question': q})\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c0632a8-b463-43e9-91fe-d7d7d8135260",
      "metadata": {
        "id": "8c0632a8-b463-43e9-91fe-d7d7d8135260",
        "outputId": "9bace925-d525-45b0-ca3b-e84daa241838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ./rag_powered_by_google_search.pdf\n"
          ]
        }
      ],
      "source": [
        "data = load_document('./rag_powered_by_google_search.pdf')\n",
        "chunks = chunk_data(data, chunk_size=256)\n",
        "vector_store = create_embeddings_chroma(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e143255-faac-4806-abdb-c2b55b82bf95",
      "metadata": {
        "scrolled": true,
        "id": "5e143255-faac-4806-abdb-c2b55b82bf95",
        "outputId": "def6fe3c-5ea7-4af8-aca3-03c67ab9152f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'How many pairs of questions and answers had the StackOverflow dataset?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?'), AIMessage(content='The StackOverflow dataset mentioned in the context had 8 million pairs of questions and answers.')], 'answer': 'The StackOverflow dataset mentioned in the context had 8 million pairs of questions and answers.'}\n"
          ]
        }
      ],
      "source": [
        "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
        "result = ask_question(q, crc)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bfda3a7-409c-4511-a698-0165b6acad80",
      "metadata": {
        "id": "6bfda3a7-409c-4511-a698-0165b6acad80",
        "outputId": "ed29d635-9cf2-41b8-be31-ebd3bfd232ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The StackOverflow dataset mentioned in the context had 8 million pairs of questions and answers.\n"
          ]
        }
      ],
      "source": [
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cb937c3-8769-44af-9afe-fc49c3d5a1f2",
      "metadata": {
        "scrolled": true,
        "id": "9cb937c3-8769-44af-9afe-fc49c3d5a1f2"
      },
      "outputs": [],
      "source": [
        "q = 'Multiply that number by 10.'\n",
        "result = ask_question(q, crc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1848f53-68b5-4265-bf0c-f02b50028fb5",
      "metadata": {
        "id": "f1848f53-68b5-4265-bf0c-f02b50028fb5",
        "outputId": "29e4483d-ae3e-4f21-d6b6-8e90434c5d41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The result of multiplying the number of pairs of questions and answers in the StackOverflow dataset by 10 would be 80 million pairs.\n"
          ]
        }
      ],
      "source": [
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0120ceea-f701-44de-838c-a4edec180c55",
      "metadata": {
        "scrolled": true,
        "id": "0120ceea-f701-44de-838c-a4edec180c55",
        "outputId": "5406be42-d7a3-485b-bd1c-35a100715dac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The result of dividing 80 million pairs by 80 is 1 million.\n"
          ]
        }
      ],
      "source": [
        "q = 'Divide the result by 80.'\n",
        "result = ask_question(q, crc)\n",
        "print(result['answer'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f373d4-13fb-4ee9-9c22-c0072a24be2f",
      "metadata": {
        "id": "39f373d4-13fb-4ee9-9c22-c0072a24be2f",
        "outputId": "95b55ab6-f414-4ebf-d774-309be54f8056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='How many pairs of questions and answers had the StackOverflow dataset?'\n",
            "content='The StackOverflow dataset mentioned in the context had 8 million pairs of questions and answers.'\n",
            "content='Multiply that number by 10.'\n",
            "content='The result of multiplying the number of pairs of questions and answers in the StackOverflow dataset by 10 would be 80 million pairs.'\n",
            "content='Divide the result by 80.'\n",
            "content='The result of dividing 80 million pairs by 80 is 1 million.'\n"
          ]
        }
      ],
      "source": [
        "for item in result['chat_history']:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8c84af-793b-4f40-ba8d-ed57170788e2",
      "metadata": {
        "id": "5b8c84af-793b-4f40-ba8d-ed57170788e2"
      },
      "source": [
        "### Loop for asking questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efcd92ad-cc2e-47b0-b8da-112aa8a4c432",
      "metadata": {
        "id": "efcd92ad-cc2e-47b0-b8da-112aa8a4c432",
        "outputId": "1f1a682c-3339-4c35-b57d-c56420972bd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your question: tell me about google search\n",
            "Google Search is a product that enables search experiences for public or internal websites, mobile applications, and enterprise search services. It allows users to quickly find highly relevant documents and content to get the information they need in seconds. Google has invested significantly in developing this search technology to reduce costs and latency, bringing production-grade semantic search experiences to billions of users.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Your question: is vertex ai useful compared to others?\n",
            "Vertex AI offers advanced generative AI capabilities, enterprise-ready features, customizable answers, search tuning, vector search, grounding, and compliance updates for enterprises. It also leverages Tensor Processing Unit (TPU) for large-scale semantic search. This makes Vertex AI a powerful tool for businesses looking to harness the power of AI and machine learning. However, to determine if Vertex AI is more useful compared to other products, a detailed comparison with specific alternatives would be necessary.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Your question: bye\n",
            "Bye bye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    q = input('Your question: ')\n",
        "    if q.lower() in 'exit quit bye':\n",
        "        print('Bye bye!')\n",
        "        break\n",
        "    result = ask_question(q, crc)\n",
        "    print(result['answer'])\n",
        "    print('-' * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "808125d7-5026-4771-b7ec-72b8b35c42dc",
      "metadata": {
        "id": "808125d7-5026-4771-b7ec-72b8b35c42dc"
      },
      "source": [
        "## Using a Custom Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "825f97e1-eb73-4fb6-b253-13e564dd6240",
      "metadata": {
        "id": "825f97e1-eb73-4fb6-b253-13e564dd6240"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
        "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "\n",
        "system_template = r'''\n",
        "Use the following pieces of context to answer the user's question.\n",
        "Before answering translate your response to Spanish.\n",
        "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
        "---------------\n",
        "Context: ```{context}```\n",
        "'''\n",
        "\n",
        "user_template = '''\n",
        "Question: ```{question}```\n",
        "'''\n",
        "\n",
        "messages= [\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(user_template)\n",
        "]\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "crc = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    chain_type='stuff',\n",
        "    combine_docs_chain_kwargs={'prompt': qa_prompt },\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5667186-1562-4c68-b479-725824f9974b",
      "metadata": {
        "id": "a5667186-1562-4c68-b479-725824f9974b",
        "outputId": "5f06b5b7-e8c0-4043-e6bf-635d861afdc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['context', 'question'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='\\nUse the following pieces of context to answer the user\\'s question.\\nBefore answering translate your response to Spanish.\\nIf you don\\'t find the answer in the provided context, just respond \"I don\\'t know.\"\\n---------------\\nContext: ```{context}```\\n')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='\\nQuestion: ```{question}```\\n'))]\n"
          ]
        }
      ],
      "source": [
        "print(qa_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0df828fa-8d84-4753-8a0a-dc11d4f1f320",
      "metadata": {
        "id": "0df828fa-8d84-4753-8a0a-dc11d4f1f320",
        "outputId": "9a8d4ff4-74dc-44d0-ce8a-e269744fdc6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: \n",
            "Use the following pieces of context to answer the user's question.\n",
            "Before answering translate your response to Spanish.\n",
            "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
            "---------------\n",
            "Context: ```simple similarity search was highly e\u0000ective because the dataset had 8\n",
            "million pairs of questions and answers. However, datasets do not\n",
            "usually contain pre-existing question-and-answer or query-and-\n",
            "\n",
            "simple similarity search was highly e\u0000ective because the dataset had 8\n",
            "million pairs of questions and answers. However, datasets do not\n",
            "usually contain pre-existing question-and-answer or query-and-\n",
            "\n",
            "distinctly di\u0000erent meanings. Why, then, do you use similarity search to\n",
            "\u0000nd answers?\n",
            "Semantic search is not just similarity\n",
            "search\n",
            "In the Stack Ove\u0000low demo that we introduced in a previous post,\n",
            "\n",
            "distinctly di\u0000erent meanings. Why, then, do you use similarity search to\n",
            "\u0000nd answers?\n",
            "Semantic search is not just similarity\n",
            "search\n",
            "In the Stack Ove\u0000low demo that we introduced in a previous post,\n",
            "\n",
            "candidate pairs in many real-world RAG scenarios. Therefore, it is vital\n",
            "for an AI model to learn and be able to predict the relationship between\n",
            "queries and their corresponding answers to deliver production-quality\n",
            "semantic search.```\n",
            "\n",
            "Human: \n",
            "Question: ```How many pairs of questions and answers had the StackOverflow dataset?```\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'question': 'How many pairs of questions and answers had the StackOverflow dataset?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?'), AIMessage(content='El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\\n\\nEl conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.')], 'answer': 'El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\\n\\nEl conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.'}\n"
          ]
        }
      ],
      "source": [
        "db = load_embeddings_chroma()\n",
        "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
        "result = ask_question(q, crc)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffdef3a8-07ff-4671-9502-096d8e8655ae",
      "metadata": {
        "id": "ffdef3a8-07ff-4671-9502-096d8e8655ae",
        "outputId": "a15d6d10-4510-4397-8c1c-701ac19d4b01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
            "Assistant: El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\n",
            "\n",
            "El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\n",
            "Follow Up Input: When was Elon Musk born?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: \n",
            "Use the following pieces of context to answer the user's question.\n",
            "Before answering translate your response to Spanish.\n",
            "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
            "---------------\n",
            "Context: ```Follow usTelecommunications\n",
            "By Ankur Jain • 7-minute readGaming\n",
            "By Patrick Smith • 8-minute read\n",
            "Application Development\n",
            "By Mete Atamel • 5-minute read\n",
            "AI & Machine Learning\n",
            "By Burak Gokturk • 3-minute readMWC’24: Unlocking the AI-enabled Telco with\n",
            "\n",
            "Follow usTelecommunications\n",
            "By Ankur Jain • 7-minute readGaming\n",
            "By Patrick Smith • 8-minute read\n",
            "Application Development\n",
            "By Mete Atamel • 5-minute read\n",
            "AI & Machine Learning\n",
            "By Burak Gokturk • 3-minute readMWC’24: Unlocking the AI-enabled Telco with\n",
            "\n",
            "house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
            "TPUs are speci\u0000cally tailored to provide the underlying power needed\n",
            "to suppo\u0000 machine learning and AI workloads, but their genesis is\n",
            "\n",
            "house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
            "TPUs are speci\u0000cally tailored to provide the underlying power needed\n",
            "to suppo\u0000 machine learning and AI workloads, but their genesis is\n",
            "\n",
            "stumped. But with neural matching, we’re able to make sense of it.\n",
            "By looking at the broader representations of concepts in the query\n",
            "— management, leadership, personality and more — neural\n",
            "matching can decipher that this searcher is looking for```\n",
            "\n",
            "Human: \n",
            "Question: ```¿Cuándo nació Elon Musk?```\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'question': 'When was Elon Musk born?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?'), AIMessage(content='El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\\n\\nEl conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.'), HumanMessage(content='When was Elon Musk born?'), AIMessage(content='No sé.')], 'answer': 'No sé.'}\n"
          ]
        }
      ],
      "source": [
        "q = 'When was Elon Musk born?'\n",
        "result = ask_question(q, crc)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e556f294-1ff1-47cb-b83b-cc2cee79c712",
      "metadata": {
        "id": "e556f294-1ff1-47cb-b83b-cc2cee79c712",
        "outputId": "f2ab4793-fb7a-4229-8912-c3feb02fc6b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
            "Assistant: El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\n",
            "\n",
            "Contexto: ```la búsqueda de similitud simple fue altamente efectiva porque el conjunto de datos tenía 8 millones de pares de preguntas y respuestas. Sin embargo, los conjuntos de datos generalmente no contienen preguntas y respuestas preexistentes o consultas y respuestas preexistentes.```\n",
            "Human: When was Elon Musk born?\n",
            "Assistant: I don't know.\n",
            "Follow Up Input: When was Bill Gates born?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: \n",
            "Use the following pieces of context to answer the user's question.\n",
            "Before answering translate your response to Spanish.\n",
            "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
            "---------------\n",
            "Context: ```simple similarity search was highly e\u0000ective because the dataset had 8\n",
            "million pairs of questions and answers. However, datasets do not\n",
            "usually contain pre-existing question-and-answer or query-and-\n",
            "\n",
            "simple similarity search was highly e\u0000ective because the dataset had 8\n",
            "million pairs of questions and answers. However, datasets do not\n",
            "usually contain pre-existing question-and-answer or query-and-\n",
            "\n",
            "simple similarity search was highly e\u0000ective because the dataset had 8\n",
            "million pairs of questions and answers. However, datasets do not\n",
            "usually contain pre-existing question-and-answer or query-and-\n",
            "\n",
            "simple similarity search was highly e\u0000ective because the dataset had 8\n",
            "million pairs of questions and answers. However, datasets do not\n",
            "usually contain pre-existing question-and-answer or query-and-\n",
            "\n",
            "simple similarity search was highly e\u0000ective because the dataset had 8\n",
            "million pairs of questions and answers. However, datasets do not\n",
            "usually contain pre-existing question-and-answer or query-and-```\n",
            "\n",
            "Human: \n",
            "Question: ```¿Cuántos pares de preguntas y respuestas tenía el conjunto de datos de StackOverflow?```\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'question': 'When was Bill Gates born?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?'), AIMessage(content='El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\\n\\nContexto: ```la búsqueda de similitud simple fue altamente efectiva porque el conjunto de datos tenía 8 millones de pares de preguntas y respuestas. Sin embargo, los conjuntos de datos generalmente no contienen preguntas y respuestas preexistentes o consultas y respuestas preexistentes.```'), HumanMessage(content='When was Elon Musk born?'), AIMessage(content=\"I don't know.\"), HumanMessage(content='When was Bill Gates born?'), AIMessage(content='No sé.')], 'answer': 'No sé.'}\n"
          ]
        }
      ],
      "source": [
        "q = 'When was Bill Gates born?'\n",
        "result = ask_question(q, crc)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c08ea5c3-8a5f-4447-8d40-dd13150fb6eb",
      "metadata": {
        "id": "c08ea5c3-8a5f-4447-8d40-dd13150fb6eb",
        "outputId": "be23c3d2-6425-4c44-fd14-9ad0cea71677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No sé.\n"
          ]
        }
      ],
      "source": [
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4822533-ed90-4adb-8c53-13d605b6bb2b",
      "metadata": {
        "id": "b4822533-ed90-4adb-8c53-13d605b6bb2b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}